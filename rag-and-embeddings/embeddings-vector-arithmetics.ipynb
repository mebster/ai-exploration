{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30883cd-0964-4f34-bda9-e8f3a3d6b0df",
   "metadata": {},
   "source": [
    "# Embeddings vector arithmetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9cc2b5-58b2-43fa-85e0-f3722199b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00448464  0.01810506 -0.06230397  0.15338692 -0.11522211  0.11418289\n",
      "  0.07872835  0.0495244   0.10117409  0.00895918  0.00329942 -0.18080738\n",
      " -0.03512578 -0.10034303 -0.06474461  0.10942298  0.04074675  0.09414344\n",
      " -0.12549578 -0.04378415 -0.011129   -0.11219814 -0.09034315  0.12971823\n",
      " -0.16390741]\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class Words(Enum):\n",
    "    king = 0\n",
    "    man = 1\n",
    "    woman = 2\n",
    "    queen = 3\n",
    "    philosophy = 4\n",
    "\n",
    "words: List[str] = [Words.king.name, \n",
    "                    Words.man.name, \n",
    "                    Words.woman.name, \n",
    "                    Words.queen.name,\n",
    "                    Words.philosophy.name]\n",
    "\n",
    "embeddings: List[float] = []\n",
    "\n",
    "# Load a pre-trained model for embeddings\n",
    "model = SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6') # 65MB in RAM?\n",
    "\n",
    "for word in words:\n",
    "    embedding = model.encode(word)  # Encode the chunk and take the first element\n",
    "    embeddings.append(np.array(embedding))\n",
    "\n",
    "queen_res = embeddings[Words.king.value] - embeddings[Words.man.value] + embeddings[Words.woman.value]\n",
    "print(queen_res[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b64146-7130-420e-b442-bbed63f3b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    '''Compute the cosine similarity between two vectors.'''\n",
    "    return dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f046c3c0-235d-46b4-841f-e261feee6673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57947886"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(queen_res, embeddings[Words.queen.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1b12c4-9f17-4b9d-a224-ef2bede11f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>king</th>\n",
       "      <th>man</th>\n",
       "      <th>woman</th>\n",
       "      <th>queen</th>\n",
       "      <th>philosophy</th>\n",
       "      <th>queen bis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>king</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.321646</td>\n",
       "      <td>0.263995</td>\n",
       "      <td>0.680713</td>\n",
       "      <td>0.177250</td>\n",
       "      <td>0.630572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.321646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.325679</td>\n",
       "      <td>0.254117</td>\n",
       "      <td>0.177514</td>\n",
       "      <td>-0.235992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.263995</td>\n",
       "      <td>0.325679</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439399</td>\n",
       "      <td>0.259879</td>\n",
       "      <td>0.627873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queen</th>\n",
       "      <td>0.680713</td>\n",
       "      <td>0.254117</td>\n",
       "      <td>0.439399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161236</td>\n",
       "      <td>0.579479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>philosophy</th>\n",
       "      <td>0.177250</td>\n",
       "      <td>0.177514</td>\n",
       "      <td>0.259879</td>\n",
       "      <td>0.161236</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queen bis</th>\n",
       "      <td>0.630572</td>\n",
       "      <td>-0.235992</td>\n",
       "      <td>0.627873</td>\n",
       "      <td>0.579479</td>\n",
       "      <td>0.173721</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                king       man     woman     queen  philosophy  queen bis\n",
       "king        1.000000  0.321646  0.263995  0.680713    0.177250   0.630572\n",
       "man         0.321646  1.000000  0.325679  0.254117    0.177514  -0.235992\n",
       "woman       0.263995  0.325679  1.000000  0.439399    0.259879   0.627873\n",
       "queen       0.680713  0.254117  0.439399  1.000000    0.161236   0.579479\n",
       "philosophy  0.177250  0.177514  0.259879  0.161236    1.000000   0.173721\n",
       "queen bis   0.630572 -0.235992  0.627873  0.579479    0.173721   1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "\n",
    "_embeddings = embeddings.copy()\n",
    "_embeddings.append(queen_res)\n",
    "n = len(_embeddings)\n",
    "similarity_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        similarity_matrix[i][j] = cosine_similarity(_embeddings[i], _embeddings[j])\n",
    "\n",
    "# Display results in a table\n",
    "index_labels = [w.name for w in Words]\n",
    "index_labels.append(\"queen bis\")\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=index_labels, columns=index_labels)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fdd1f-2c23-4e50-a185-61c4c38c0a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
